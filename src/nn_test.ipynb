{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statistics import mean\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchsummary import summary\n",
    "\n",
    "from dataloader import *\n",
    "sys.path.append('../')\n",
    "from pre_process import *\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/tommaselli/Documents/IC_NN_Lidar\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ArtificialLidarDatasetCNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m CSV_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./artificial_data/tags/Artificial_Label_Data3.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m TRAIN_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./artificial_data/train3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 30\u001b[0m train_data, val_data \u001b[38;5;241m=\u001b[39m \u001b[43mgetData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCSV_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTRAIN_PATH\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 14\u001b[0m, in \u001b[0;36mgetData\u001b[0;34m(csv_path, train_path, batch_size, num_workers)\u001b[0m\n\u001b[1;32m     11\u001b[0m val_data  \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,num_workers\u001b[38;5;241m=\u001b[39mnum_workers)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# get one image shape from the train_data\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_data):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataset.py:298\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[0;32m--> 298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/Documents/IC_NN_Lidar/src/dataloader.py:96\u001b[0m, in \u001b[0;36mNnDataLoader.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     93\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage \u001b[38;5;66;03m# just for now\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m#* PROCESS LABELs\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43mArtificialLidarDatasetCNN\u001b[49m\u001b[38;5;241m.\u001b[39mprocess_label(labels)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# labels_dep = PreProcess.deprocess(image=self.image, label=labels)        \u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# m1, m2, b1, b2 = labels_dep\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# x1 = np.arange(0, image.shape[0], 1)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    109\u001b[0m \n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m#! suppose m1 = m2\u001b[39;00m\n\u001b[1;32m    111\u001b[0m w1, w2, q1, q2 \u001b[38;5;241m=\u001b[39m labels\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ArtificialLidarDatasetCNN' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def getData(csv_path, train_path, batch_size=7, num_workers=0):\n",
    "    ''' get images from the folder (assets/images) and return a DataLoader object '''\n",
    "    \n",
    "    dataset = NnDataLoader(csv_path, train_path)\n",
    "\n",
    "    train_size, val_size = int(0.8*len(dataset)), np.ceil(0.2*len(dataset)).astype('int')\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_data = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,num_workers=num_workers)\n",
    "    val_data  = DataLoader(val_dataset, batch_size=batch_size, shuffle=True,num_workers=num_workers)\n",
    "\n",
    "    # get one image shape from the train_data\n",
    "    for i, data in enumerate(train_data):\n",
    "        print(f'images shape: {data[\"image\"].shape}')\n",
    "        print(f'label length: {len(data[\"labels\"])}')\n",
    "        break\n",
    "    print('-'*65)\n",
    "\n",
    "    print(f'train size: {train_size}, val size: {val_size}')\n",
    "    return train_data, val_data\n",
    "\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print(\"Current working directory:\", cwd)\n",
    "\n",
    "\n",
    "CSV_PATH = \"./artificial_data/tags/Artificial_Label_Data3.csv\"\n",
    "TRAIN_PATH = \"./artificial_data/train3\"\n",
    "train_data, val_data = getData(csv_path=CSV_PATH, train_path=TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model with the validation data for one random image\n",
    "# showing the image and the predicted and real labels\n",
    "# get the first image from the validation data\n",
    "for i, data in enumerate(train_data):\n",
    "    images = data['image']\n",
    "    labels = data['labels']\n",
    "    print('l:', labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image dimension: (batch, channels, height, width)\n",
    "images = images.type(torch.float32).to(device)\n",
    "images = images.unsqueeze(1)\n",
    "\n",
    "labels = [label.type(torch.float32).to(device) for label in labels]\n",
    "# convert labels to tensor\n",
    "labels = torch.stack(labels)\n",
    "# convert to format: tensor([[value1, value2, value3, value4], [value1, value2, value3, value4], ...])\n",
    "# this is: labels for each image, \"batch\" times -> shape: (batch, 4)\n",
    "labels = labels.permute(1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############ MODEL ############\n",
    "# load the model from the saved file\n",
    "model = models.resnet18()\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "# Adding batch normalization and an additional convolutional layer\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(num_ftrs, 512),\n",
    "    nn.BatchNorm1d(512),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(512, 256),\n",
    "    nn.BatchNorm1d(256),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(256, 3)  # Alterado para 3 valores de saída\n",
    ")\n",
    "\n",
    "# Moving the model to the device (GPU/CPU)\n",
    "model = model.to(device)\n",
    "path = os.getcwd() + '/models/' + '/model_005_27-10-2023_23-16-15.pth'\n",
    "model.load_state_dict(torch.load(path))\n",
    "model.eval()\n",
    "\n",
    "# image it is the first image from the images batch\n",
    "image = images[0].unsqueeze(0)\n",
    "# label it is the first label from the labels batch\n",
    "label = labels[0].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Inicie a contagem de tempo antes da inferência\n",
    "start_time = time.time()\n",
    "\n",
    "# get the model predictions\n",
    "predictions = model(image)\n",
    "\n",
    "# Encerre a contagem de tempo após a inferência\n",
    "end_time = time.time()\n",
    "\n",
    "print('Inference time: {:.4f} ms'.format((end_time - start_time)*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the predictions to numpy array\n",
    "predictions = predictions.to('cpu').cpu().detach().numpy()\n",
    "# convert the labels to numpy array\n",
    "label = labels.to('cpu').cpu().detach().numpy()\n",
    "\n",
    "\n",
    "# convert image to cpu \n",
    "image = image.to('cpu').cpu().detach().numpy()\n",
    "# image it is shape (1, 1, 507, 507), we need to remove the first dimension\n",
    "image = image[0][0]\n",
    "\n",
    "# print the predictions and labels\n",
    "print('predictions:', predictions)\n",
    "print('label:', label[0])\n",
    "\n",
    "print('>>> image shape:', image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = PreProcess.deprocess(image=image, label=label[0].tolist())\n",
    "predictions = PreProcess.deprocess(image=image, label=predictions[0].tolist())\n",
    "\n",
    "print('label (deprocessed):', label)\n",
    "print('predictions (deprocessed):', predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line equations explicitly\n",
    "\n",
    "# get the x coordinates\n",
    "x = np.arange(0, 224)\n",
    "\n",
    "# get the slopes and intercepts\n",
    "m1, m2, b1, b2 = label\n",
    "\n",
    "# get the x and y coordinates of the lines\n",
    "y1 = m1*x + b1\n",
    "y2 = m2*x + b2\n",
    "\n",
    "# get the slopes and intercepts\n",
    "m1p, m2p, b1p, b2p = predictions\n",
    "\n",
    "# get the x and y coordinates of the lines\n",
    "y1p = m1p*x + b1p\n",
    "y2p = m2p*x + b2p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 5), frameon=True)\n",
    "\n",
    "# plot the lines\n",
    "ax.plot(x, y1, color='red', label='Real')\n",
    "ax.plot(x, y2, color='red')\n",
    "\n",
    "\n",
    "ax.plot(x, y1p, color='blue', label='Predicted')\n",
    "ax.plot(x, y2p, color='blue')\n",
    "\n",
    "# Customize the title\n",
    "title_font = {'family': 'serif', 'color': 'black', 'weight': 'bold', 'size': 18}\n",
    "plt.title('Inference Results', fontdict=title_font)\n",
    "\n",
    "\n",
    "# Add a white rectangle to create a border\n",
    "border_style = dict(facecolor='none', edgecolor='black', linewidth=2)\n",
    "ax.add_patch(plt.Rectangle((0, 0), 1, 1, **border_style, transform=ax.transAxes))\n",
    "\n",
    "# legend\n",
    "plt.legend(loc='upper right', prop={'size': 8, 'weight': 'bold', 'family': 'serif'})\n",
    "\n",
    "# show the image\n",
    "ax.imshow(image, cmap='inferno')\n",
    "ax.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
